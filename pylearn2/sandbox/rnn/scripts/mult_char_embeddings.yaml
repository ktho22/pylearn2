!obj:pylearn2.train.Train {
  dataset: !obj:pylearn2.sandbox.nlp.datasets.word2vec.Word2Vec {
    which_set: 'train',
    stop: 100000
  },
  model: !obj:pylearn2.models.mlp.MLP {
    input_space: !obj:pylearn2.sandbox.rnn.space.SequenceSpace {
      space: !obj:pylearn2.space.IndexSpace {
        dim: 1,
        max_labels: 101,
      },
    },
    layers: [
      !obj:pylearn2.sandbox.rnn.models.rnn.MultiplicativeRUGatedRecurrent {
      #!obj:pylearn2.sandbox.rnn.models.mlp.Multiplicative_Gated_Recurrent {
        layer_name: 'recurrent_layer',
        proj_dim : 300,
        dim: 500,
        max_labels: 101,
        irange: 0.01,
        indices: [-1]
      },
      !obj:pylearn2.models.mlp.Linear {
        layer_name: 'linear_layer',
        dim: 300,
        irange: 0.01,
       # use_cosine_loss: True
      }
    ],
  },
  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
    learning_rate: .000001,
    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.AdaDelta {},
    batch_size:  32,
    monitoring_dataset: {
      valid: !obj:pylearn2.sandbox.nlp.datasets.word2vec.Word2Vec {
        which_set: 'valid',
        stop: 1000
      },
      train: !obj:pylearn2.sandbox.nlp.datasets.word2vec.Word2Vec {
        which_set: 'train',
        stop: 1000
      },
    },
    cost: !obj:pylearn2.sandbox.rnn.costs.gradient_clipping.GradientClipping {
      clipping_value: 1,
      cost: !obj:pylearn2.costs.mlp.Default {}
    }
  },
  save_path: '/Tmp/devincol/mult_char_embeddings_masked_test.pkl',
  save_freq: 1,
}
