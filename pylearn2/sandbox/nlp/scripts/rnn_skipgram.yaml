
!obj:pylearn2.train.Train {
  dataset: !obj:pylearn2.sandbox.nlp.datasets.rnn_skipgram.H5RnnSkipgram {
    path: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/binarized_text.en.h5',
    node: 'none', #('/phrases', '/indices'),
    which_set: 'train',
    frame_length: &n 7,
    word_dict: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/vocab.en.pkl',
    char_dict: '/data/lisatmp3/devincol/data/translation_char_vocab.en.pkl',
    stop: 10000000,
    load_to_memory: True,
    _iter_num_batches: 3000,
    word_labels: &w 30000,
    char_labels: &c 145,
    cache_size: 100000,
    cache_delta: 10000,
    schwenk: True,
  },
  model: !obj:pylearn2.models.mlp.MLP {
    input_space: !obj:pylearn2.sandbox.rnn.space.SequenceSpace {
      space: !obj:pylearn2.space.IndexSpace {
        dim: 1,
        max_labels: *c # Number of different characters + 1 for eow
      },
    },
    target_source: 'targets', #['target0', 'target1', 'target2', 'target3', 'target4', 'target5' ],
    layers: [
     # !obj:pylearn2.sandbox.nlp.models.mlp.ProjectionLayer {
     #   layer_name: 'projection',
     #   dim: 256,
     #   irange: 0.01
     #  },
     #!obj:pylearn2.sandbox.rnn.models.rnn.Recurrent {
     !obj:pylearn2.sandbox.rnn.models.rnn.FactoredMultiplicativeRUGatedRecurrent {
        layer_name: 'recurrent_layer',
        proj_dim:  256,
        dim: 300,
        max_labels: *c,
        irange: 0.01,
        indices: [-1]
      },
      !obj:pylearn2.models.mlp.Linear {
        layer_name: 'outputEmbedding',
        dim: *w,
        irange: 0.01
      },
      !obj:pylearn2.sandbox.nlp.models.mlp.Softmax {
        n_classes: *w,
        layer_name: 'h0',
        irange: 0.01,
        binary_target_dim: 1,
        no_affine: True,
      },
    ],
  },
  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
    batch_size: 32, #256,
    learning_rate: .000001,
    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.AdaDelta {},
    monitoring_dataset: {
      'valid': !obj:pylearn2.sandbox.nlp.datasets.rnn_skipgram.H5RnnSkipgram {
        path: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/binarized_text.en.h5',
        node: 'none', #('/phrases', '/indices'),
        which_set: 'valid',
        frame_length: *n,
        word_dict: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/vocab.en.pkl',
        char_dict: '/data/lisatmp3/devincol/data/translation_char_vocab.en.pkl',
        start: 10000000,
        stop: 10005000,
        load_to_memory: True,
        _iter_num_batches: 150,
        word_labels: *w,
        char_labels: *c,
        schwenk: True,
      },
     'train': !obj:pylearn2.sandbox.nlp.datasets.rnn_skipgram.H5RnnSkipgram {
        path: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/binarized_text.en.h5',
        node: 'none', #('/phrases', '/indices'),
        which_set: 'valid',
        frame_length: *n,
        word_dict: '/data/lisatmp3/chokyun/mt/vocab.30k/bitexts.selected/vocab.en.pkl',
        char_dict: '/data/lisatmp3/devincol/data/translation_char_vocab.en.pkl',
        start: 100000,
        stop: 105000,
        load_to_memory: True,
        _iter_num_batches: 150,
        word_labels: *w,
        char_labels: *c,
        schwenk: True,
      
     },
    },
    cost: !obj:pylearn2.sandbox.rnn.costs.gradient_clipping.GradientClipping {
      clipping_value: 1,
      cost: !obj:pylearn2.costs.mlp.Default {}
    }

  },
  #extensions: [
  #     !obj:pylearn2.train_extensions.WordRelationship {
  #       vocab: "/data/lisatmp3/pougetj/vocab.pkl",
  #       questions: "/data/lisatmp3/pougetj/questions-words.txt",
  #       vocab_size: *w,
  #       UNK: 1,
  #       n_batches: 4, 
  #       use_chars: True,
  #       char_dict_path: '/data/lisa/data/word2vec/char_vocab.pkl',
  #     }
  #],
  save_path: '/Tmp/devincol/rnn_realskipgram_factored_schwenk_256_300_Ada.pkl',
  save_freq: 1,
}
